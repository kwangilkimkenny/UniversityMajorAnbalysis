{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import docx2txt\n",
    "import sys\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#docx 문서들을 불러와서 txt 로 변환한다.\n",
    "txtdata = docx2txt.process(\"UniversityMajor/01_Harvard/HarvardCS40COMMUNITY-03.docx\") \n",
    "with open(\"UniversityMajor/01_Harvard/HarvardEssays.txt\", \"w\", encoding='UTF8') as text_file:\n",
    "    print(txtdata, file=text_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "UniversityMajor/01_Harvard/HarvardPS45P2-04.docx\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0xd2 in position 16: invalid continuation byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-e38de45bddd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mout_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mout_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/venv/lib/python3.6/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    319\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0xd2 in position 16: invalid continuation byte"
     ]
    }
   ],
   "source": [
    "#directory = \"UniversityMajor/01_Harvard/\"\n",
    "outfile_name = \"merged_essay.txt\"\n",
    "\n",
    "out_file = open(outfile_name, 'w')\n",
    "#files = os.listdir(directory)\n",
    "files = glob.glob(\"UniversityMajor/01_Harvard/*.docx\") #파일 리스트를 불러온다.\n",
    "for filename in files:\n",
    "    if \".docx\" not in filename:\n",
    "        continue\n",
    "    print(type(filename))\n",
    "    print(filename)\n",
    "    file = open(filename, encoding='utf-8')\n",
    "    for line in file:\n",
    "        out_file.write(line)\n",
    "    out_file.write(line)\n",
    "    out_file.write(\"\\n\")\n",
    "    file.close()\n",
    "out_file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached https://files.pythonhosted.org/packages/1e/27/6fdcddfbce1963989eb527f0ba4749829509c0172c275806cffd5a7e1776/gensim-3.8.1-cp36-cp36m-macosx_10_6_intel.macosx_10_9_intel.macosx_10_9_x86_64.macosx_10_10_intel.macosx_10_10_x86_64.whl\n",
      "Requirement already satisfied: numpy>=1.11.3 in /anaconda3/lib/python3.6/site-packages (from gensim) (1.16.4)\n",
      "Collecting scipy>=0.18.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/94/cd76305a69fff844e83655ed7b254835df4eddd5fc0e2d0eb2914501b36e/scipy-1.4.1-cp36-cp36m-macosx_10_6_intel.whl (28.5MB)\n",
      "\u001b[K     |████████████████████████████████| 28.5MB 4.9MB/s eta 0:00:01     |█████████████████████▏          | 18.9MB 8.3MB/s eta 0:00:02     |█████████████████████▌          | 19.1MB 8.3MB/s eta 0:00:02\n",
      "\u001b[?25hCollecting smart-open>=1.8.1 (from gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/14/47cf88d290e4681be35f3b6e8889ba26ed809a0ba14336dc8ae46ffcfda8/smart_open-1.10.0.tar.gz (99kB)\n",
      "\u001b[K     |████████████████████████████████| 102kB 4.9MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5.0 in /Users/kimkwangil/.local/lib/python3.6/site-packages (from gensim) (1.14.0)\n",
      "Requirement already satisfied: requests in /anaconda3/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim) (2.21.0)\n",
      "Collecting boto3 (from smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b5/27c94221d240b9df187b2623243b22db914238a651549defa59ffbfed7b4/boto3-1.12.24-py2.py3-none-any.whl (128kB)\n",
      "\u001b[K     |████████████████████████████████| 133kB 7.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting google-cloud-storage (from smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/88/67/80761781f813ffbf8bc1db7270b6d23de7a96468da4601de3bf2e5e1d829/google_cloud_storage-1.26.0-py2.py3-none-any.whl (75kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 9.6MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: chardet<3.1.0,>=3.0.2 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2.8)\n",
      "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (1.24.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /anaconda3/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim) (2019.9.11)\n",
      "Collecting jmespath<1.0.0,>=0.7.1 (from boto3->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/a3/43/1e939e1fcd87b827fe192d0c9fc25b48c5b3368902bfb913de7754b0dc03/jmespath-0.9.5-py2.py3-none-any.whl\n",
      "Collecting botocore<1.16.0,>=1.15.24 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/aa/001b6c820b44dd5fcf7361ddad9be4f1cbc4609f5487583f74bcf1b93001/botocore-1.15.24-py2.py3-none-any.whl (6.0MB)\n",
      "\u001b[K     |████████████████████████████████| 6.0MB 6.5MB/s eta 0:00:01     |██████████████▊                 | 2.7MB 1.4MB/s eta 0:00:03\n",
      "\u001b[?25hCollecting s3transfer<0.4.0,>=0.3.0 (from boto3->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/79/e6afb3d8b0b4e96cefbdc690f741d7dd24547ff1f94240c997a26fa908d3/s3transfer-0.3.3-py2.py3-none-any.whl (69kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 5.6MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting google-auth<2.0dev,>=1.11.0 (from google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f7/f8/2da482a6165ef3f28d52faf8c2ca31628129a84a294033eb399ef500e265/google_auth-1.11.3-py2.py3-none-any.whl (76kB)\n",
      "\u001b[K     |████████████████████████████████| 81kB 4.5MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting google-cloud-core<2.0dev,>=1.2.0 (from google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/89/3c/8a7531839028c9690e6d14c650521f3bbaf26e53baaeb2784b8c3eb2fb97/google_cloud_core-1.3.0-py2.py3-none-any.whl\n",
      "Collecting google-resumable-media<0.6dev,>=0.5.0 (from google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/35/9e/f73325d0466ce5bdc36333f1aeb2892ead7b76e79bdb5c8b0493961fa098/google_resumable_media-0.5.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /anaconda3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3->smart-open>=1.8.1->gensim) (0.14)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /anaconda3/lib/python3.6/site-packages (from botocore<1.16.0,>=1.15.24->boto3->smart-open>=1.8.1->gensim) (2.7.5)\n",
      "Collecting rsa<4.1,>=3.1.4 (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/02/e5/38518af393f7c214357079ce67a317307936896e961e35450b70fad2a9cf/rsa-4.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: setuptools>=40.3.0 in /anaconda3/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (40.6.3)\n",
      "Collecting cachetools<5.0,>=2.0.0 (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/08/6a/abf83cb951617793fd49c98cb9456860f5df66ff89883c8660aa0672d425/cachetools-4.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /anaconda3/lib/python3.6/site-packages (from google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.2.2)\n",
      "Collecting google-api-core<2.0.0dev,>=1.16.0 (from google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/7e/a523169b0cc9ce62d56e07571db927286a94b1a5f51ac220bd97db825c77/google_api_core-1.16.0-py2.py3-none-any.whl (70kB)\n",
      "\u001b[K     |████████████████████████████████| 71kB 5.2MB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: pyasn1>=0.1.3 in /anaconda3/lib/python3.6/site-packages (from rsa<4.1,>=3.1.4->google-auth<2.0dev,>=1.11.0->google-cloud-storage->smart-open>=1.8.1->gensim) (0.4.4)\n",
      "Requirement already satisfied: pytz in /anaconda3/lib/python3.6/site-packages (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim) (2018.7)\n",
      "Collecting protobuf>=3.4.0 (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/65/3ac73d6a9f31de4b45ebff6885e9a4ccd16eccb63764dd406140d337fabd/protobuf-3.11.3-cp36-cp36m-macosx_10_9_x86_64.whl (1.3MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3MB 38kB/s eta 0:00:012\n",
      "\u001b[?25hCollecting googleapis-common-protos<2.0dev,>=1.6.0 (from google-api-core<2.0.0dev,>=1.16.0->google-cloud-core<2.0dev,>=1.2.0->google-cloud-storage->smart-open>=1.8.1->gensim)\n",
      "  Downloading https://files.pythonhosted.org/packages/05/46/168fd780f594a4d61122f7f3dc0561686084319ad73b4febbf02ae8b32cf/googleapis-common-protos-1.51.0.tar.gz\n",
      "Building wheels for collected packages: smart-open, googleapis-common-protos\n",
      "  Building wheel for smart-open (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/kimkwangil/Library/Caches/pip/wheels/f8/00/d4/a6b8b6aa241459103d19e757f96645549dd562d5b5de653f44\n",
      "  Building wheel for googleapis-common-protos (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/kimkwangil/Library/Caches/pip/wheels/2c/f9/7f/6eb87e636072bf467e25348bbeb96849333e6a080dca78f706\n",
      "Successfully built smart-open googleapis-common-protos\n",
      "Installing collected packages: scipy, jmespath, botocore, s3transfer, boto3, rsa, cachetools, google-auth, protobuf, googleapis-common-protos, google-api-core, google-cloud-core, google-resumable-media, google-cloud-storage, smart-open, gensim\n",
      "Successfully installed boto3-1.12.24 botocore-1.15.24 cachetools-4.0.0 gensim-3.8.1 google-api-core-1.16.0 google-auth-1.11.3 google-cloud-core-1.3.0 google-cloud-storage-1.26.0 google-resumable-media-0.5.0 googleapis-common-protos-1.51.0 jmespath-0.9.5 protobuf-3.11.3 rsa-4.0 s3transfer-0.3.3 scipy-1.4.1 smart-open-1.10.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "import nltk #형태소 분석기 가져오기\n",
    "import string #특수문자 \n",
    "import warnings#경고알림문자 제거\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning) #경고알림 무시"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "EOFError",
     "evalue": "Ran out of input",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEOFError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-89181c3ffb71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m#함수를 불러오는 메인코드\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0minput_file_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"UniversityMajor/01_Harvard/HarvardEssays.txt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mdocuments\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_documents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0mSW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_stopwords\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storwords.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mcleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext_cleaning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-89181c3ffb71>\u001b[0m in \u001b[0;36mread_documents\u001b[0;34m(input_file_name)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_file_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mtemp_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_corpus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mEOFError\u001b[0m: Ran out of input"
     ]
    }
   ],
   "source": [
    "def read_documents(input_file_name):\n",
    "    \n",
    "    corpus = []\n",
    "    \n",
    "    with open(input_file_name, 'rb') as f:\n",
    "        temp_corpus = pickle.load(f)\n",
    "    \n",
    "    for page in temp_corpus:\n",
    "        corpus += page\n",
    "        \n",
    "    return corpus\n",
    "\n",
    "def text_cleaning(doc):\n",
    "    cleaned_docs = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        temp_doc = re.sub(\"\", doc)\n",
    "        cleaned_docs.append(temp_doc)\n",
    "        \n",
    "    return cleaned_docs\n",
    "\n",
    "def define_stopwords(path):\n",
    "    SW = set()\n",
    "    for i in string.punctuation:\n",
    "        SW.add(i)\n",
    "    with open(path) as f:\n",
    "        for word in f:\n",
    "            SW.add(word)\n",
    "    return SW\n",
    "\n",
    "def text_tokenizing(corpus, tokenizer):\n",
    "    token_corpus = []\n",
    "    \n",
    "    if tokenizer == \"none\":\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = nltk.nouns(corpus[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            token_corpus.append(token_text)\n",
    "            \n",
    "    elif tokenized == \"morph\":\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = nltk.nouns(corpus[n])\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            token_corpus.append(token_text)\n",
    "            \n",
    "    elif tokenized == \"word\":\n",
    "        for n in tqdm_notebook(range(len(corpus)), desc=\"Preprocessing\"):\n",
    "            token_text = corpus[n].split()\n",
    "            token_text = [word for word in token_text if word not in SW and len(word) > 1]\n",
    "            token_corpus.append(token_text)\n",
    "\n",
    "                \n",
    "    return token_corpus\n",
    "\n",
    "#함수를 불러오는 메인코드\n",
    "input_file_name = \"UniversityMajor/01_Harvard/HarvardEssays.txt\"\n",
    "documents = read_documents(input_file_name)\n",
    "SW = define_stopwords(\"storwords.txt\")\n",
    "cleaned_text = text_cleaning(documents)\n",
    "tokenized_text = text_tokenizing(cleaned_text, tokenizer=\"noun\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-d3ab8c71ee33>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'UniversityMajor/01_Harvard/HarvardEssays.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    683\u001b[0m         )\n\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1135\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1136\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/venv/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "#load the dataset\n",
    "dataset = pandas.read_csv('UniversityMajor/01_Harvard/HarvardEssays.txt', delimiter='\\t')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
